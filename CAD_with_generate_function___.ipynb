{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyOLKi8weXCm"
      },
      "source": [
        "# Context-aware decoding with generate() function\n",
        "\n",
        "## Changes in initialization within CustomGPTNeoModel class:\n",
        "\n",
        "**self.context_logits**: This stores the external logits (from a different input or context) that can be used to modify the original logits during the forward pass.<br>\n",
        "\n",
        "## Case 1: Standard Text Generation (Without Context Logits)\n",
        "- This case uses the standard generate() method from Hugging Face, without adjusting the logits with the context.\n",
        "- generated_ids_no_context = model.generate(): The model generates a sequence of tokens starting from context_ids. Several parameters are set:<br>\n",
        " **attention_mask = input_attention_mask** indicates which tokens should be attended to or ignored.<br>\n",
        " **max_length=20**: Generates up to 20 tokens in total.<br>\n",
        " **do_sample=True**: Enables random sampling from the distribution of predicted next tokens.<br>\n",
        "  **top_p=0.9**: Implements top-p sampling (nucleus sampling) where only the top 90% probable tokens are considered for generation.<br>\n",
        "   **temperature=0.7**: Controls the randomness of predictions. A lower temperature results in less random outputs.\n",
        "\n",
        "- **The generated sequence is decoded into text**:<br> tokenizer.decode(generated_ids_no_context[0], skip_special_tokens=True).\n",
        "\n",
        "## Case 2: Custom Model with Context-Aware Logit Adjustment\n",
        "\n",
        "- The model uses the same generate() method, but internally adjusts the logits based on the context_logits (if provided). The process of generating the sequence remains the same as in Case 1, but this time the logits are influenced by the context text in a more controlled way.\n",
        "\n",
        "- **The generated sequence is again decoded into tex**t:<br> tokenizer.decode(generated_ids_with_context[0], skip_special_tokens=True)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_Voz36zvIzL9"
      },
      "outputs": [],
      "source": [
        "from time import time\n",
        "import torch\n",
        "from transformers import GPTNeoForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jjSsAEWvI6pE"
      },
      "outputs": [],
      "source": [
        "class CustomGPTNeoModel(GPTNeoForCausalLM):\n",
        "    def __init__(self, config, alpha=1):\n",
        "        super().__init__(config)\n",
        "        self.alpha = alpha\n",
        "        self.context_logits = None\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, **kwargs):\n",
        "        original_outputs = super().forward(input_ids, attention_mask=attention_mask, **kwargs)\n",
        "        original_logits = original_outputs.logits\n",
        "\n",
        "        if self.context_logits is None:\n",
        "            return original_outputs  # Return regular output if no context\n",
        "\n",
        "        # Direct logit manipulation as described in the paper\n",
        "        adjusted_logits = (1 + self.alpha) * self.context_logits - self.alpha * original_logits\n",
        "\n",
        "        # Apply softmax only after the adjustment, across the token dimension (-1)\n",
        "        adjusted_logits = torch.softmax(adjusted_logits, dim=-1)\n",
        "\n",
        "        # Return adjusted logits\n",
        "        original_outputs.logits = adjusted_logits\n",
        "        return original_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "jT7K7yU-JFdy"
      },
      "outputs": [],
      "source": [
        "# Input and context texts\n",
        "input_text = \"Argentina has won FIFA world cups in years:\"\n",
        "context_text = \"Argentina won world cups in 1978, 1986, 2022\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "qwrzfmixJJ2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4141164-eb95-47bc-c24f-13d7773ba5b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Initialize the tokenizer and original GPT-Neo model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "\n",
        "# Encode input and context\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "context_ids = tokenizer(context_text + input_text, return_tensors=\"pt\", padding=True).input_ids\n",
        "\n",
        "\n",
        "input_attention_mask=tokenizer(input_text, return_tensors=\"pt\").attention_mask\n",
        "context_attention_mask=tokenizer(context_text + input_text, return_tensors=\"pt\", padding=True).attention_mask\n",
        "\n",
        "# Get the context logits by running the model on the context + input\n",
        "with torch.no_grad():\n",
        "    context_outputs = model.forward(context_ids)\n",
        "    context_logits = context_outputs.logits[:, -1, :]\n",
        "    input_logits = model.forward(input_ids).logits[:, -1, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGU439zD7R6k"
      },
      "source": [
        "## Case 1: Without context adjustment using generate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPQmy81IJQxp",
        "outputId": "6c523629-f0f4-441a-baf2-8e32a01c48c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time without context: 726.671808719635\n",
            "No context\n",
            "Generated text (without context-aware decoding): Argentina has won FIFA world cups in years: 2001, 2002, 2003, 2006, 2007\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create an instance of the custom model\n",
        "custom_model = CustomGPTNeoModel.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "custom_model.alpha = 1  # alpha can be adjusted\n",
        "\n",
        "start = time()\n",
        "generated_ids_no_context = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=input_attention_mask,\n",
        "    max_length=20,\n",
        "    num_return_sequences=1,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        "    temperature=0.7\n",
        ")\n",
        "end = time()\n",
        "print(f\"Time without context: {end-start}\")\n",
        "print(\"No context\")\n",
        "print(f\"Generated text (without context-aware decoding): {tokenizer.decode(generated_ids_no_context[0], skip_special_tokens=True)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4SlRAJ8ewbq"
      },
      "source": [
        "## Case 2: Using the custom model and assinging context_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EZH04KuJv0r",
        "outputId": "430d2c55-8d1e-4ce7-cd85-61b838a997c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Time with context-aware model: 631.3298809528351\n",
            "Generated text using generate() with context-aware logits adjustment:\n",
            "Argentina won world cups in 1978, 1986, 2022Argentina has won FIFA world cups in years: 1978, 1986, 2022.Ar\n"
          ]
        }
      ],
      "source": [
        "# Text generation using regular generate() method, but with context and adjusted logits\n",
        "custom_model = CustomGPTNeoModel.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "custom_model.alpha = 1  # alpha can be adjusted\n",
        "custom_model.context_logits = context_logits  # Passing the context logits to the model\n",
        "\n",
        "start = time()\n",
        "generated_ids_with_context = custom_model.generate(\n",
        "    input_ids=context_ids,\n",
        "    max_length=30,\n",
        "    attention_mask=context_attention_mask,\n",
        "    num_return_sequences=1,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        "    temperature=0.7\n",
        ")\n",
        "end = time()\n",
        "\n",
        "print(f\"\\nTime with context-aware model: {end-start}\")\n",
        "print(\"Generated text using generate() with context-aware logits adjustment:\")\n",
        "print(f\"{tokenizer.decode(generated_ids_with_context[0], skip_special_tokens=True)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rz4OViRnLyPZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}